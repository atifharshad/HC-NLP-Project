# -*- coding: utf-8 -*-
"""Distilbert_LoRA_under_DomainShift.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dd6YEk_CDISM4YkX7gwjUgxz08s0308F

## **DistilBERT for Sentiment Analysis under Domain Shift**

**Overview**

This project implements sentiment analysis using DistilBERT, a lightweight language model, with LoRA (Low-Rank Adaptation) for efficient fine-tuning. The model is trained on the IMDb dataset (in-domain) and evaluated on both IMDb and Amazon datasets (out-of-domain) to assess its performance under domain shift. The notebook uses DistilBERT to explore its capabilities in sentiment classification.

**Objectives:**

* Fine-tune DistilBERT for binary sentiment classification (positive/negative).
* Evaluate performance metrics (accuracy, precision, recall, F1 score) on in-domain (IMDb) and out-of-domain (Amazon) datasets.
* Assess trustworthiness using Brier Score and Expected Calibration Error (ECE).
* Quantify the impact of domain shift by comparing performance metrics across datasets.

**Hardware Requirements**

* **GPU:** A GPU (e.g., NVIDIA T4, as specified in the notebook's metadata) is recommended for efficient training and evaluation.

**Dependencies**

Install the required Python packages using:
## 1. Setup & Dependencies

This section installs necessary libraries and imports them for the project.

* `!pip install -q transformers datasets scikit-learn matplotlib accelerate peft`: Installs key libraries: `transformers` for models, `datasets` for data loading, `scikit-learn` for metrics, `matplotlib` for plotting, `accelerate` for hardware optimization, and `peft` for LoRA.
* `import ...`: Imports specific modules and functions from the installed libraries for tasks like data handling, model loading, training, evaluation, and plotting.
* `def set_seed(seed: int = 42): ...`: Defines a function to set random seeds for reproducibility across different libraries (Python, NumPy, PyTorch).
* `set_seed()`: Calls the `set_seed` function to initialize the random state.
"""

!pip install -q transformers datasets scikit-learn matplotlib accelerate peft

import os
import random
import numpy as np
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
    EarlyStoppingCallback
)
from peft import get_peft_config, get_peft_model, LoraConfig
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, brier_score_loss
from sklearn.calibration import calibration_curve, CalibrationDisplay
import matplotlib.pyplot as plt

# Set a random seed for reproducibility
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed()
print(f"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}")

"""## 2. Data Preparation

Purpose: Loads and preprocesses the IMDb and Amazon datasets for training and evaluation.

Details:

Datasets:

* IMDb: Used for training and in-domain validation. Typically loaded using the datasets library (e.g., load_dataset("imdb")).
* Amazon: Used for out-of-domain evaluation, represented by the train-00000-of-00001.parquet file (21.0MB).

Role:

* Prepares text data for model training and evaluation.
* Ensures compatibility with DistilBERT's input requirements (e.g., tokenized sequences).


* **`imdb = load_dataset("imdb")`**: Loads the IMDb dataset.
* **`amazon = load_dataset("amazon_polarity")`**: Loads the Amazon Polarity dataset.
* **`def sample_split(ds, label, n): ...`**: Defines a function to create balanced subsets by filtering for a specific label, shuffling, and selecting a fixed number of samples.
* The code then uses `sample_split` to create balanced training, IMDb test, and Amazon test sets, each with an equal number of positive and negative samples.  The training set uses 12,500 positive and 12,500 negative reviews, for a total of 25,000. The test sets use 1000 positive and 1000 negative reviews, for a total of 2,000 each.
* **Dataset Combination**: The code combines the positive and negative subsets using `concatenate_datasets` and shuffles the result.
* **`def rename_content(example): ...`**: Defines a function to rename the 'content' column in the Amazon dataset to 'text' for consistency with the IMDb dataset.
* **`amazon_test = amazon_test.map(rename_content)`**: Applies the `rename_content` function to the Amazon test set.

"""

from datasets import concatenate_datasets

imdb = load_dataset("imdb")
amazon = load_dataset("amazon_polarity")

# Create balanced subsets
def sample_split(ds, label, n):
    return ds.filter(lambda x: x['label'] == label).shuffle(seed=42).select(range(n))

# 5k for training (2.5k pos, 2.5k neg)
pos_train = sample_split(imdb['train'], 1, 12500)
neg_train = sample_split(imdb['train'], 0, 12500)
train_ds = concatenate_datasets([pos_train, neg_train]).shuffle(seed=42)

# 1k for test (1000 pos, 1000 neg)
pos_test = sample_split(imdb['test'], 1, 1000)
neg_test = sample_split(imdb['test'], 0, 1000)
imdb_test = concatenate_datasets([pos_test, neg_test]).shuffle(seed=42)

# Amazon domain-shift test: 1000 pos, 1000 neg
amazon_pos = sample_split(amazon['test'], 1, 1000)
amazon_neg = sample_split(amazon['test'], 0, 1000)
amazon_test = concatenate_datasets([amazon_pos, amazon_neg]).shuffle(seed=42)

# Rename Amazon 'content' to 'text' for consistency
def rename_content(example):
    return { 'text': example['content'], 'label': example['label'] }
amazon_test = amazon_test.map(rename_content)

print(f"Train size: {len(train_ds)}, IMDb test: {len(imdb_test)}, Amazon test: {len(amazon_test)}")

"""## 3. Tokenization & Preprocessing

This section prepares the text data for the model by tokenizing it and converting it into a numerical format.

Preprocessing (inferred from typical sentiment analysis pipelines):

* Tokenizes text using DistilBERT's tokenizer.
* Applies truncation and padding to ensure uniform input lengths.
* Converts labels to binary format (0 for negative, 1 for positive).

* **`MODEL_NAME = "distilbert-base-uncased"`**: Specifies the pre-trained model to use (DistilBERT).
* **`MAX_LEN = 512`**: Defines the maximum sequence length for tokenization.
* **`tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)`**: Loads the tokenizer associated with the DistilBERT model.
* **`tokenizer.pad_token = ...`**: Sets the pad token for the tokenizer.
* **`def preprocess(batch): ...`**: Defines a function to:
    * Tokenize the text using the loaded tokenizer with truncation and padding to `MAX_LEN`.
    * Assign the text's label to the tokenized output.
* **`train_enc = train_ds.map(preprocess, batched=True)`**: Applies the `preprocess` function to the training dataset.
* **`imdb_enc = imdb_test.map(preprocess, batched=True)`**: Applies the `preprocess` function to the IMDb test dataset.
* **`amazon_enc = amazon_test.map(preprocess, batched=True)`**: Applies the `preprocess` function to the Amazon test dataset.
* **Removing Text Columns**: The code removes the original text columns ('text') from the encoded datasets, as they are no longer needed after tokenization.

"""

MODEL_NAME = "distilbert-base-uncased"
MAX_LEN = 512

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token

# Preprocess function: tokenize and attach labels

def preprocess(batch):
    enc = tokenizer(
        batch['text'], truncation=True, padding='max_length', max_length=MAX_LEN
    )
    enc['labels'] = batch['label']
    return enc

train_enc = train_ds.map(preprocess, batched=True)
imdb_enc = imdb_test.map(preprocess, batched=True)
amazon_enc = amazon_test.map(preprocess, batched=True)

# Remove raw columns
cols = ['text']
train_enc = train_enc.remove_columns(cols)
imdb_enc = imdb_enc.remove_columns(cols)
amazon_enc = amazon_enc.remove_columns(cols)

"""## 4. Model Configuration and LoRA Setup

**Purpose**: Configures the DistilBERT model and applies LoRA for efficient fine-tuning.

**Details**:

* **Model**: DistilBERT, a compact language model, adapted for sentiment classification.
* **LoRA**:
    * Low-Rank Adaptation reduces the number of trainable parameters by adding low-rank matrices to specific layers (e.g., attention).
    * Improves training efficiency, especially on resource-constrained environments.
* **Configuration**:
    * Loads the model using transformers.AutoModelForSequenceClassification.
    * Applies LoRA configuration using peft (Parameter-Efficient Fine-Tuning library).
    * Sets hyperparameters (e.g., learning rate, batch size, epochs).
* **Code Explanation**:
    * **`base_model`**: Loads the pre-trained DistilBERT model for sequence classification with 2 output labels (positive/negative).
    * **`lora_config`**: Defines the LoRA configuration:
        * `r`: Rank of the LoRA adaptation matrices (16).
        * `lora_alpha`: Scaling factor for the LoRA weights (32).
        * `target_modules`: The modules to apply LoRA to ("q_lin", "k_lin", "v_lin" in DistilBERT).
        * `lora_dropout`: Dropout probability for LoRA layers (0.1).
        * `bias`: Specifies how bias is handled (none).
        * `task_type`: The type of task ("SEQ_CLS" for sequence classification).
    * **`model`**: Wraps the base DistilBERT model with LoRA using the defined configuration.
    * **`model.print_trainable_parameters()`**: Prints the number of trainable parameters in the LoRA-adapted model.

"""

base_model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=2
)

# from peft import get_peft_model, LoraConfig

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_lin", "k_lin", "v_lin"],
    lora_dropout=0.1,
    bias="none",
    task_type="SEQ_CLS"
)

model = get_peft_model(base_model, lora_config)
model.print_trainable_parameters()

"""## 5. Training

**Purpose**: Fine-tunes the DistilBERT model on the IMDb dataset using the Trainer API.

**Details**:

* **Training Process**:

    * Uses the transformers.Trainer for managing training loops, optimization, and checkpointing.
    * Trains on the tokenized IMDb dataset.
    * Monitors metrics like training loss and validation accuracy.
* **Hyperparameters**:
    * Learning rate: \~5e-5 (typical for LoRA fine-tuning).
    * Batch size: \~16 or 32 (based on GPU constraints).
    * Epochs: \~3-5 (common for sentiment tasks).
* **Evidence**:
    * The evaluation output (e.g., loss: 0.2081 for IMDb) suggests successful training.
* **Code Explanation**:
    * **`training_args`**: Configures training parameters:
        * `output_dir`: Directory to save model checkpoints.
        * `num_train_epochs`: Number of training epochs (4).
        * `per_device_train_batch_size`: Training batch size per GPU (16).
        * `gradient_accumulation_steps`: Accumulate gradients to simulate larger batches.
        * `per_device_eval_batch_size`: Evaluation batch size per GPU (64).
        * `eval_strategy`, `eval_steps`: Evaluate at every 100 steps.
        * `save_strategy`, `save_steps`: Save checkpoints every 100 steps.
        * `learning_rate`: Learning rate for the optimizer (5e-5).
        * `weight_decay`: L2 regularization to prevent overfitting (0.01).
        * `load_best_model_at_end`: Load the best model at the end of training.
        * `metric_for_best_model`: Metric to determine the best model ("f1").
        * `logging_dir`, `logging_steps`: Directory and frequency for logging.
        * `fp16`: Enable mixed-precision training for speedup.
        * `seed`: Random seed for reproducibility.
        * `report_to`: Disable reporting to any external service.
    * **`compute_metrics(p)`**: Calculates evaluation metrics (accuracy, precision, recall, F1 score).
    * **`data_collator`**: Handles padding of input sequences.
    * **`trainer`**: Initializes the Hugging Face `Trainer` for model training and evaluation, using the specified model, training arguments, datasets, tokenizer, data collator, metrics, and early stopping.

"""

training_args = TrainingArguments(
    output_dir = "./distilbert_lora",
    num_train_epochs = 4,
    per_device_train_batch_size = 16,
    gradient_accumulation_steps = 1,
    per_device_eval_batch_size = 64,
    eval_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    learning_rate       = 5e-5,
    weight_decay        = 0.01,
    load_best_model_at_end = True,
    metric_for_best_model = "f1",
    logging_dir          = "./logs",
    logging_steps        = 100,
    fp16                 = True,
    seed                 = 42,
    report_to            = "none"
)

def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    return {
        'accuracy': accuracy_score(labels, preds),
        'precision': precision_score(labels, preds),
        'recall': recall_score(labels, preds),
        'f1': f1_score(labels, preds)
    }

data_collator = DataCollatorWithPadding(tokenizer)

trainer = Trainer(
    model = model,
    args  = training_args,
    train_dataset = train_enc,
    eval_dataset  = imdb_enc,
    tokenizer     = tokenizer,
    data_collator = DataCollatorWithPadding(tokenizer),
    compute_metrics = compute_metrics,
    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]
)

print("Starting fine-tuning...")
trainer.train()
trainer.save_model()
print("Model saved.")

"""# **Learning Curves**"""

import matplotlib.pyplot as plt

# --- Extract metrics from trainer state
hist = trainer.state.log_history

train_steps = [x['step'] for x in hist if 'loss' in x and 'eval_loss' not in x]
train_loss  = [x['loss'] for x in hist if 'loss' in x and 'eval_loss' not in x]
val_steps   = [x['step'] for x in hist if 'eval_loss' in x]
val_loss    = [x['eval_loss'] for x in hist if 'eval_loss' in x]
val_f1      = [x['eval_f1'] for x in hist if 'eval_f1' in x]
val_acc     = [x['eval_accuracy'] for x in hist if 'eval_accuracy' in x]


# --- Plot 1: Loss vs Step
plt.figure(figsize=(8, 5))
plt.plot(train_steps, train_loss, label='Train Loss')
plt.plot(val_steps, val_loss,   label='Validation Loss')
plt.xlabel('Training Step')
plt.ylabel('Loss')
plt.title('Loss vs Step')
plt.legend()
plt.grid(True)
plt.show()

# --- Plot 2: Validation F1 vs Step
plt.figure(figsize=(8, 5))
plt.plot(val_steps, val_f1, label='Validation F1')
plt.xlabel('Training Step')
plt.ylabel('F1 Score')
plt.title('Validation F1 vs Step')
plt.legend()
plt.grid(True)
plt.show()

# --- Plot 3: Validation Accuracy vs Step
plt.figure(figsize=(8, 5))
plt.plot(val_steps, val_acc, label='Validation Accuracy')
plt.xlabel('Training Step')
plt.ylabel('Accuracy')
plt.title('Validation Accuracy vs Step')
plt.legend()
plt.grid(True)
plt.show()

"""## **Confusion Matrix**"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

class_names = ['negative', 'positive']

def plot_cm(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(6,5))
    sns.heatmap(cm_norm,
                annot=cm,
                fmt='g',
                cmap='Blues',
                xticklabels=class_names,
                yticklabels=class_names)
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.title(title)
    plt.show()

print("IMDb Test Set Results")
preds_imdb = trainer.predict(imdb_enc)
y_pred_imdb = np.argmax(preds_imdb.predictions, axis=1)
y_true_imdb = preds_imdb.label_ids

print(classification_report(y_true_imdb, y_pred_imdb,
                            target_names=class_names))
plot_cm(y_true_imdb, y_pred_imdb, title="IMDb Confusion Matrix")

# --- Amazon test set (domain-shift) ---
print("Amazon Polarity Test Set Results")
preds_amazon = trainer.predict(amazon_enc)
y_pred_amzn = np.argmax(preds_amazon.predictions, axis=1)
y_true_amzn = preds_amazon.label_ids

print(classification_report(y_true_amzn, y_pred_amzn,
                            target_names=class_names))
plot_cm(y_true_amzn, y_pred_amzn, title="Amazon Confusion Matrix")

"""# Domain Shift"""

import numpy as np
import matplotlib.pyplot as plt

imdb_metrics = trainer.evaluate(imdb_enc)
ood_metrics  = trainer.evaluate(amazon_enc)   # replace `book_enc` with `amazon_enc` if you prefer


metrics = ['accuracy', 'precision', 'recall', 'f1']
imdb_vals = np.array([imdb_metrics[f"eval_{m}"] for m in metrics])
ood_vals  = np.array([ood_metrics[f"eval_{m}"] for m in metrics])

drops_abs = imdb_vals - ood_vals
drops_pct = drops_abs / imdb_vals * 100

print("\n=== Performance Summary ===")
print(f"{'Metric':<10} | {'IMDb':>6} | {'OOD':>6} | {'Δ (abs)':>8} | {'Δ (%)':>7}")
print("-" * 45)
for name, im, oo, da, dp in zip(metrics, imdb_vals, ood_vals, drops_abs, drops_pct):
    print(f"{name.capitalize():<10} | {im:6.3f} | {oo:6.3f} | {da:8.3f} | {dp:7.1f}%")
print()

x = np.arange(len(metrics))
width = 0.3

plt.figure(figsize=(10, 5))
plt.bar(x - width/2 - 0.02, imdb_vals, width, label='IMDb (in-domain)')
plt.bar(x + width/2 + 0.02, ood_vals, width, label='Out-of-domain')
plt.xticks(x, [m.capitalize() for m in metrics])
plt.ylim(0, 1)
plt.ylabel('Score')
plt.title('In-Domain vs Out-of-Domain Performance')
plt.legend(loc="upper center", bbox_to_anchor=(0.5, -0.1), ncol=2, frameon=False)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
plt.bar([m.capitalize() for m in metrics], drops_pct, color='salmon')
plt.ylabel('% Drop from IMDb')
plt.title('Domain-Shift Impact')
plt.ylim(min(0, drops_pct.min() * 1.2), drops_pct.max() * 1.2)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

"""# Bias individual"""

import numpy as np
import scipy.special
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


def get_predictions_and_probs(dataset_enc, raw_texts):
    preds_probs = trainer.predict(dataset_enc)
    preds = np.argmax(preds_probs.predictions, axis=1)
    probs = scipy.special.softmax(preds_probs.predictions, axis=1)[:, 1]
    return preds, probs, preds_probs.label_ids


amazon_preds, amazon_probs, amazon_true = get_predictions_and_probs(amazon_enc, amazon_test["text"])

keywords = ["love", "hate", "funny", "boring", "excellent", "terrible"]
keyword_results = {}

print("\nKeyword-based Bias Analysis on Amazon dataset:")
for word in keywords:
    idx_with    = [i for i, txt in enumerate(amazon_test["text"]) if word in txt.lower()]
    idx_without = [i for i in range(len(amazon_test["text"])) if i not in idx_with]

    for grp_name, idx in [("mentions_"+word, idx_with), ("no_"+word, idx_without)]:
        if len(idx) < 10:
            print(f"Skipping {grp_name} (n={len(idx)}) — too few samples")
            continue

        sub_preds = amazon_preds[idx]
        sub_true  = amazon_true[idx]
        metrics = {
            "accuracy" : accuracy_score(sub_true, sub_preds),
            "precision": precision_score(sub_true, sub_preds, zero_division=0),
            "recall"   : recall_score(sub_true, sub_preds, zero_division=0),
            "f1"       : f1_score(sub_true, sub_preds, zero_division=0),
            "pos_ratio": np.mean(sub_preds)
        }
        keyword_results.setdefault(word, {})[grp_name] = metrics

        print(f"\nMetrics for {grp_name} (n={len(idx)}):")
        print(f"  Accuracy : {metrics['accuracy']:.4f}")
        print(f"  Precision: {metrics['precision']:.4f}")
        print(f"  Recall   : {metrics['recall']:.4f}")
        print(f"  F1-score : {metrics['f1']:.4f}")
        print(f"  Pos Ratio: {metrics['pos_ratio']:.4f}")

bias_data = []
for word, groups in keyword_results.items():
    m1 = groups.get("mentions_"+word)
    m2 = groups.get("no_"+word)
    if m1 and m2:
        bias = m1["pos_ratio"] - m2["pos_ratio"]
        bias_data.append((word, bias, len(idx_with), len(idx_without)))


bias_data.sort(key=lambda x: abs(x[1]), reverse=True)

if bias_data:
    words, biases, _, _ = zip(*bias_data)
    colors = ["green" if b > 0 else "red" for b in biases]
    plt.figure(figsize=(10, 6))
    bars = plt.bar(words, biases, color=colors)
    plt.axhline(0, color="black", linestyle="--", alpha=0.5)
    plt.ylabel("Bias (Δ positive-prediction rate)")
    plt.title("Keyword Bias Analysis on Amazon Reviews")
    plt.grid(axis="y", linestyle="--", alpha=0.3)
    plt.show()
else:
    print("Not enough data for bias visualization.")

import numpy as np
import scipy.special
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# --- 1. Predict & get probabilities function ---
def get_predictions_and_probs(dataset_enc):
    preds_probs = trainer.predict(dataset_enc)
    preds = np.argmax(preds_probs.predictions, axis=1)
    probs = scipy.special.softmax(preds_probs.predictions, axis=1)[:, 1]
    return preds, probs, preds_probs.label_ids

# Run on IMDb
imdb_preds, imdb_probs, imdb_true = get_predictions_and_probs(imdb_enc)

# --- 2. Set up keywords & storage ---
keywords = ["love", "hate", "funny", "boring", "excellent", "terrible"]
keyword_results_imdb = {}

print("\nKeyword-based Bias Analysis on IMDb dataset:")
for word in keywords:
    idx_with    = [i for i, txt in enumerate(imdb_test["text"]) if word in txt.lower()]
    idx_without = [i for i in range(len(imdb_test)) if i not in idx_with]

    for grp_name, idx in [("mentions_"+word, idx_with), ("no_"+word, idx_without)]:
        if len(idx) < 10:
            print(f"Skipping {grp_name} (n={len(idx)}) — too few samples")
            continue

        sub_preds = imdb_preds[idx]
        sub_true  = imdb_true[idx]
        metrics = {
            "accuracy" : accuracy_score(sub_true, sub_preds),
            "precision": precision_score(sub_true, sub_preds, zero_division=0),
            "recall"   : recall_score(sub_true, sub_preds, zero_division=0),
            "f1"       : f1_score(sub_true, sub_preds, zero_division=0),
            "pos_ratio": np.mean(sub_preds)
        }
        keyword_results_imdb.setdefault(word, {})[grp_name] = metrics

        print(f"\nMetrics for {grp_name} (n={len(idx)}):")
        print(f"  Accuracy : {metrics['accuracy']:.4f}")
        print(f"  Precision: {metrics['precision']:.4f}")
        print(f"  Recall   : {metrics['recall']:.4f}")
        print(f"  F1-score : {metrics['f1']:.4f}")
        print(f"  Pos Ratio: {metrics['pos_ratio']:.4f}")

bias_data_imdb = []
for word, groups in keyword_results_imdb.items():
    m1 = groups.get("mentions_"+word)
    m2 = groups.get("no_"+word)
    if m1 and m2:
        bias = m1["pos_ratio"] - m2["pos_ratio"]
        bias_data_imdb.append((word, bias, len(idx_with), len(idx_without)))

bias_data_imdb.sort(key=lambda x: abs(x[1]), reverse=True)

if bias_data_imdb:
    words, biases, _, _ = zip(*bias_data_imdb)
    colors = ["green" if b > 0 else "red" for b in biases]
    plt.figure(figsize=(10, 6))
    plt.bar(words, biases, color=colors)
    plt.axhline(0, color="black", linestyle="--", alpha=0.5)
    plt.ylabel("Bias (Δ positive-prediction rate)")
    plt.title("Keyword Bias Analysis on IMDb Reviews")
    plt.grid(axis="y", linestyle="--", alpha=0.3)
    plt.show()
else:
    print("Not enough data for IMDb bias visualization.")

"""# Bias combined"""

import numpy as np
import scipy.special
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def get_preds_and_probs(dataset_enc):
    out = trainer.predict(dataset_enc)
    preds = np.argmax(out.predictions, axis=1)
    probs = scipy.special.softmax(out.predictions, axis=1)[:, 1]
    return preds, probs, out.label_ids


datasets = {
    "IMDb":  (imdb_enc, imdb_test["text"]),
    "Amazon": (amazon_enc, amazon_test["text"])
}
keywords = ["love", "hate", "funny", "boring", "excellent", "terrible"]


results = {}
for name, (enc, texts) in datasets.items():
    preds, probs, truth = get_preds_and_probs(enc)
    results[name] = {"preds": preds, "truth": truth, "texts": texts}

bias_tab = []
for name, data in results.items():
    preds, truth, texts = data["preds"], data["truth"], data["texts"]
    for kw in keywords:
        idx_w = [i for i, t in enumerate(texts) if kw in t.lower()]
        idx_wo = [i for i in range(len(texts)) if i not in idx_w]
        if len(idx_w) < 20 or len(idx_wo) < 20:
            continue
        pr_w  = preds[idx_w].mean()
        pr_wo = preds[idx_wo].mean()
        bias  = pr_w - pr_wo
        bias_tab.append((name, kw, pr_w, pr_wo, bias))

print(f"{'Dataset':<8} {'Keyword':<10} {'With':>5} {'Without':>8} {'Bias':>7}")
print("-"*40)
for ds, kw, w, wo, b in bias_tab:
    print(f"{ds:<8} {kw:<10} {w:5.2f} {wo:8.2f} {b:7.2f}")
print()

fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)
for ax, (ds_name) in zip(axes, datasets):
    subset = [(kw, b) for (d, kw, *_ , b) in bias_tab if d==ds_name]
    if not subset:
        ax.set_title(f"{ds_name}: no data")
        continue
    kws, bs = zip(*subset)
    colors = ['green' if b>0 else 'red' for b in bs]
    ax.bar(kws, bs, color=colors)
    ax.axhline(0, color='black', linestyle='--', alpha=0.5)
    ax.set_title(f"{ds_name} Bias (Δ positive rate)")
    ax.set_ylabel("Δ Pos-Pred Rate")
    ax.set_xticklabels(kws, rotation=45)
    ax.grid(axis='y', linestyle='--', alpha=0.3)

plt.tight_layout()
plt.show()

"""# Trustworthiness"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import brier_score_loss
from sklearn.calibration import calibration_curve, CalibrationDisplay


print("\n===== Trustworthiness Evaluation (Calibration) =====")

brier = brier_score_loss(amazon_true, amazon_probs)
print(f"Brier score on Amazon test: {brier:.4f}")

true_frac, pred_frac = calibration_curve(amazon_true, amazon_probs, n_bins=10, strategy="uniform")

print("\nCalibration Curve Values:")
for i, (t, p) in enumerate(zip(true_frac, pred_frac)):
    print(f" Bin {i:2d}: avg ŷ={p:.2f} → actual positive rate={t:.2f}")

plt.figure(figsize=(8, 6))
disp = CalibrationDisplay.from_predictions(
    amazon_true, amazon_probs, n_bins=10, name="DistilBERT+LoRA"
)
plt.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
plt.title("Reliability Diagram – Amazon Reviews")
plt.xlabel("Mean Predicted Probability")
plt.ylabel("Observed Frequency")
plt.legend()
plt.grid(alpha=0.3)
plt.show()

def compute_ece(y_true, y_prob, n_bins=10):
    """Compute Expected Calibration Error."""
    bin_edges = np.linspace(0, 1, n_bins + 1)
    ece = 0.0
    for lower, upper in zip(bin_edges[:-1], bin_edges[1:]):
        in_bin = (y_prob >= lower) & (y_prob < upper)
        prop_in_bin = in_bin.mean()
        if prop_in_bin > 0:
            avg_conf = y_prob[in_bin].mean()
            avg_acc  = (y_true[in_bin] == (y_prob[in_bin] >= 0.5)).mean()
            ece += prop_in_bin * abs(avg_acc - avg_conf)
    return ece

ece = compute_ece(amazon_true, amazon_probs)
print(f"\nExpected Calibration Error (ECE): {ece:.4f}")

print("\n===== Trustworthiness Evaluation (Calibration) on IMDb =====")

imdb_preds, imdb_probs, imdb_true = get_predictions_and_probs(imdb_enc)

brier_imdb = brier_score_loss(imdb_true, imdb_probs)
print(f"Brier score on IMDb test: {brier_imdb:.4f}")

true_frac_imdb, pred_frac_imdb = calibration_curve(imdb_true, imdb_probs, n_bins=10, strategy="uniform")

print("\nCalibration Curve Values (IMDb):")
for i, (t, p) in enumerate(zip(true_frac_imdb, pred_frac_imdb)):
    print(f" Bin {i:2d}: avg ŷ={p:.2f} → actual positive rate={t:.2f}")

plt.figure(figsize=(8, 6))
CalibrationDisplay.from_predictions(imdb_true, imdb_probs, n_bins=10, name="DistilBERT+LoRA (IMDb)")
plt.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
plt.title("Reliability Diagram – IMDb Test Set")
plt.xlabel("Mean Predicted Probability")
plt.ylabel("Observed Frequency")
plt.grid(alpha=0.3)
plt.legend()
plt.show()

ece_imdb = compute_ece(imdb_true, imdb_probs)
print(f"\nExpected Calibration Error (ECE) on IMDb: {ece_imdb:.4f}")

"""# Summary"""

import matplotlib.pyplot as plt

# Extract metrics from training history
history = trainer.state.log_history

train_steps = [x.get('step') for x in history if 'loss' in x and 'eval_loss' not in x]
train_loss  = [x.get('loss') for x in history if 'loss' in x and 'eval_loss' not in x]
val_steps   = [x.get('step') for x in history if 'eval_loss' in x]
val_loss    = [x.get('eval_loss') for x in history if 'eval_loss' in x]
val_f1      = [x.get('eval_f1') for x in history if 'eval_f1' in x]
val_acc     = [x.get('eval_accuracy') for x in history if 'eval_accuracy' in x]

# --- Plot training curves ---
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.plot(train_steps, train_loss, label='Train Loss')
plt.plot(val_steps, val_loss, label='Validation Loss')
plt.xlabel('Training Step')
plt.ylabel('Loss')
plt.title('Loss Curves')
plt.legend()
plt.grid(True)

plt.subplot(1, 3, 2)
plt.plot(val_steps, val_f1, label='Validation F1', color='green')
plt.xlabel('Training Step')
plt.ylabel('F1 Score')
plt.title('Validation F1 Score')
plt.legend()
plt.grid(True)

plt.subplot(1, 3, 3)
plt.plot(val_steps, val_acc, label='Validation Accuracy', color='orange')
plt.xlabel('Training Step')
plt.ylabel('Accuracy')
plt.title('Validation Accuracy')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('learning_curves.png')
plt.show()


print("\n===== Model Performance Summary =====")

imdb_results = trainer.evaluate(imdb_enc)
amazon_results = trainer.evaluate(amazon_enc)

print("\nIn-domain (IMDb) Performance:")
for key, value in imdb_results.items():
    if key.startswith('eval_'):
        print(f"  {key[5:]}: {value:.4f}")

print("\nOut-of-domain (Amazon) Performance:")
for key, value in amazon_results.items():
    if key.startswith('eval_'):
        print(f"  {key[5:]}: {value:.4f}")

print("\nTrustworthiness Metrics (Amazon):")
print(f"  Brier Score: {brier:.4f}")
print(f"  Expected Calibration Error: {ece:.4f}")

domain_shift = {}
for metric in ['accuracy', 'precision', 'recall', 'f1']:
    imdb_val = imdb_results[f'eval_{metric}']
    amazon_val = amazon_results[f'eval_{metric}']
    pct_change = ((amazon_val - imdb_val) / imdb_val) * 100
    domain_shift[metric] = pct_change

print("\nDomain Shift Impact (% change from IMDb to Amazon):")
for metric, change in domain_shift.items():
    direction = "increase" if change >= 0 else "decrease"
    print(f"  {metric.capitalize()}: {abs(change):.2f}% {direction}")

print("\n✅ Completed!")

!git config --global user.email "atif.harshad@st.ovgu.de"
!git config --global user.name "Atif Harshad"

# Commented out IPython magic to ensure Python compatibility.
# Step 1: Save your notebook to the local filesystem (skip if already saved)
# If your notebook is already in files, no need to run this again.

# Step 2: Set Git identity
!git config --global user.email "atif.harshad@st.ovgu.de"
!git config --global user.name "Atif Harshad"

# Step 3: Create a clean repo folder and move your file
# %cd /content
!rm -rf HC-NLP-Project
!mkdir HC-NLP-Project
!cp Distilbert_LoRA_under_DomainShift.ipynb HC-NLP-Project/
# %cd HC-NLP-Project

# Step 4: Initialize Git, create main branch
!git init
!git checkout -b main

# Step 5: Add and commit your notebook
!git add .
!git commit -m "Initial commit: DistilBERT under domain shift with LoRA"

!git remote add origin https://atifharshad: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@github.com/atifharshad/HC-NLP-Project.git
!git push -u origin main